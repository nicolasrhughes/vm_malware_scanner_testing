from os import path
from os import listdir
from os import stat
from Registry import Registry
import ntpath
import re
import mmap
import hashlib
from shutil import copy
from sys import stdout
from subprocess import run, PIPE


class ForensicUtility:

    MD5 = MD5_ONLY = 0
    SHA1 = SHA1_ONLY = 1
    MD5_AND_SHA1 = SHA1_AND_MD5 = 2

    # These regexes are used to parse udisksctl output to determine which loopback device to attach and where the
    # loopback device is mounted
    LOOPBACK_REGEX_STRING = r'(\/dev\/loop[0-9]+)\.$'
    MOUNT_REGEX_STRING = r'Mounted .* at ([\/\\].*[A-F0-9]*).$'

    # These regexes are used to convert %VARIABLE% format into traversable paths for Linux-mounted disk images
    APPDATA_REGEX_STRING = r'%APPDATA%'
    WINDIR_REGEX_STRING = r'%WINDIR%'
    USERPROFILE_REGEX_STRING = r'%USERPROFILE%'
    LOCAL_APPDATA_REGEX_STRING = r'%LOCALAPPDATA%'

    # This regex is used to convert Windows-style C: drive paths into traversable paths for Linux-mounted disk images
    DRIVE_REGEX_STRING = r'[A-Z]:'

    # This regex is used to find the Noriben timeline file, which is then parsed and used as input
    NORIBEN_TIMELINE_REGEX_STRING = r'Noriben_.*timeline.csv'
    
    launcher_text = '%echo off\r\n\r\n' \
                    'pushd %~dp0\r\n' \
                    'C:\\Windows\\SysWOW64\\cmd.exe /c call wscript '

    script_start_text = 'set argument_object = Wscript.Arguments\r\n' \
                        'set shell_object = Wscript.CreateObject("Wscript.Shell")\r\n\r\n' \
                        'Function SendKeysTo (process, keys)\r\n' \
                        '\tshell_object.AppActivate(process.ProcessID)\r\n' \
                        '\tshell_object.SendKeys keys\r\n' \
                        'End Function \r\n\r\n'

    script_end_text = 'SendKeysTo noriben, "^{c}"\r\n' \
                      'Wscript.sleep 60000\r\n' \
                      'shell_object.Run "Shutdown /s /f /t 15"\r\n'

    def __init__(self):
        self.logfile = stdout
        self.noriben_path = ''
        self.malware_path = ''
        self.appdata_regex = None
        self.windir_regex = None
        self.userprofile_regex = None
        self.local_appdata_regex = None

    def set_logfile(self, logfile_path):
        self.logfile = open(logfile_path, 'w')

    def set_logfile_by_directory(self, logfile_directory):
        self.logfile = open(path.join(logfile_directory, 'forensic_utility.log'), 'w')

    def flush_logfile(self):
        self.logfile.flush()

    def close_logfile(self):
        if self.logfile != stdout:
            self.logfile.flush()
            self.logfile.close()

    def close(self):
        self.close_logfile()

    def vboxmanage_clone_and_verify(self, input_file, output_file, supplied_hash=None, offset=0, mode=MD5_ONLY):
        self.logfile.write('Running vboxmanage clonehd command on file {}\n'.format(input_file))
        clone_result = run('vboxmanage clonehd {} {}'.format(input_file, output_file), shell=True,
                           encoding='utf-8', stdout=PIPE)
        if clone_result:
            if not supplied_hash:
                self.logfile.write('\tCalculating hash of input file {}\n'.format(input_file))
                original_hash = self.file_checksums_parallel(input_file, offset, mode)
                if original_hash:
                    self.logfile.write('\t\tHash of input file {} successfully calculated'
                                       ': {}\n'.format(input_file, original_hash))
                else:
                    self.logfile.write('\tFailed to calculate hash of input file {}\n'.format(input_file))
                    raise ValueError('Could not calculate input file hash value')
            else:
                self.logfile.write('\tUsing supplied hash value of {}\n'.format(supplied_hash))
                original_hash = supplied_hash
            self.logfile.write('\tCalculating hash of clone file {}\n'.format(output_file))
            clone_hash = self.file_checksums_parallel(output_file, offset, mode)
            if clone_hash:
                self.logfile.write('\t\tHash of clone file {} successfully calculated'
                                   ': {}\n'.format(output_file, clone_hash))
            else:
                self.logfile.write('\tFailed to calculate hash of input file {}\n'.format(input_file))
                raise ValueError('Could not calculate clone file hash value')
            if clone_hash == original_hash:
                return original_hash
        self.logfile.write('Failed to clone file {}\n')
        return []

    @staticmethod
    def external_checksum_string(input_file, hash_type, offset=0, read_length=0):
        if hash_type == ForensicUtility.MD5:
            command = 'md5sum'
        elif hash_type == ForensicUtility.SHA1:
            command = 'sha1sum'
        else:
            command = ''
        if read_length:
            return 'dd if={} skip={} count={} status=none iflag=skip_bytes | {}'.format(input_file, offset, read_length,
                                                                                        command)
        elif offset:
            return 'dd if={} skip={} status=none iflag=skip_bytes | {}'.format(input_file, offset, command)
        else:
            return '{} {}'.format(command, input_file)

    def external_file_checksums(self, input_file, offset=0, read_length=0, mode=MD5_ONLY):
        self.logfile.write('Attempting to appropriately invoke hashing functions on file {}\n'.format(input_file))
        output = []
        if mode != ForensicUtility.SHA1_ONLY:
            md5_result = run(self.external_checksum_string(input_file, self.MD5, offset, read_length),
                             shell=True, encoding='utf-8', stdout=PIPE)
            if md5_result != 0:
                for line in md5_result.stdout.split('\n'):
                    if line and not(line.isspace()):
                        md5_hash_match = re.match(self.MD5_SUM_REGEX, line)
                        if md5_hash_match:
                            output.append(md5_hash_match[0])
        if mode != ForensicUtility.MD5_ONLY:
            sha1_result = run(self.external_checksum_string(input_file, self.SHA1, offset, read_length),
                              shell=True, encoding='utf-8', stdout=PIPE)
            if sha1_result != 0:
                for line in sha1_result.stdout.split('\n'):
                    if line and not(line.isspace()):
                        sha1_hash_match = re.match(self.SHA1_SUM_REGEX, line)
                        if sha1_hash_match:
                            output.append(sha1_hash_match[0])
        if output:
            self.logfile.write('Hashing functions returned result {} for file {}\n'.format(output, input_file))
            return output
        self.logfile.write('Hashing functions failed for file {}\n'.format(input_file))
        return []

    @staticmethod
    def read_chunks(input_handle, chunk_size=4096, read_length=0):
        data_read = 0
        while True:
            if read_length:
                if read_length - data_read <= 0:
                    break
                elif read_length - data_read < chunk_size:
                    data = input_handle.read(read_length - data_read)
                else:
                    data = input_handle.read(chunk_size)
                data_read += chunk_size
                if not data:
                    break
                yield data
            else:
                data = input_handle.read(chunk_size)
                if not data:
                    break
                yield data

    def file_checksums_mmap_parallel(self, input_file, offset=0, read_length=0, chunk_size=1024, mode=MD5_ONLY):
        output = []
        with open(input_file, 'rb') as read_file:
            read_map = mmap.mmap(read_file.fileno(), length=read_length, offset=offset, access=mmap.PROT_READ)
            if (mode != ForensicUtility.SHA1_ONLY) and (mode != ForensicUtility.MD5_ONLY) \
                    and (mode != ForensicUtility.MD5_AND_SHA1):
                raise ValueError('Mode {} is not permitted'.format(mode))
            if mode != ForensicUtility.SHA1_ONLY:
                md5_hash = hashlib.md5()
            if mode != ForensicUtility.MD5_ONLY:
                sha1_hash = hashlib.sha1()
            for chunk in self.read_chunks(read_map, chunk_size, 0):
                if mode != ForensicUtility.SHA1_ONLY:
                    md5_hash.update(chunk)
                if mode != ForensicUtility.MD5_ONLY:
                    sha1_hash.update(chunk)
        if mode != ForensicUtility.SHA1_ONLY:
            if md5_hash.hexdigest():
                output.append(md5_hash.hexdigest())
        if mode != ForensicUtility.MD5_ONLY:
            if sha1_hash.hexdigest():
                output.append(sha1_hash.hexdigest())
        return output

    def file_checksums_parallel(self, input_file, offset=0, read_length=0, chunk_size=4096, mode=MD5_ONLY):
        output = []
        with open(input_file, 'rb') as read_file:
            read_file.seek(offset)
            if (mode != ForensicUtility.SHA1_ONLY) and (mode != ForensicUtility.MD5_ONLY) \
                    and (mode != ForensicUtility.MD5_AND_SHA1):
                raise ValueError('Mode {} is not permitted'.format(mode))
            if mode != ForensicUtility.SHA1_ONLY:
                md5_hash = hashlib.md5()
            if mode != ForensicUtility.MD5_ONLY:
                sha1_hash = hashlib.sha1()
            for chunk in self.read_chunks(read_file, chunk_size, read_length):
                if mode != ForensicUtility.SHA1_ONLY:
                    md5_hash.update(chunk)
                if mode != ForensicUtility.MD5_ONLY:
                    sha1_hash.update(chunk)
        if mode != ForensicUtility.SHA1_ONLY:
            if md5_hash.hexdigest():
                output.append(md5_hash.hexdigest())
        if mode != ForensicUtility.MD5_ONLY:
            if sha1_hash.hexdigest():
                output.append(sha1_hash.hexdigest())
        return output

    @staticmethod
    def file_checksums(input_file, offset=0, read_length=0, mode=MD5_ONLY):
        with open(input_file, 'rb') as read_file:
            output = []
            if read_length == 0:
                read_file_size = path.getsize(input_file)
                read_length = read_file_size - offset
            if mode != ForensicUtility.SHA1_ONLY:
                output.append(hashlib.md5(mmap.mmap(fileno=read_file.fileno(), length=read_length, prot=mmap.PROT_READ,
                                                    offset=offset)).hexdigest())
            if mode != ForensicUtility.MD5_ONLY:
                output.append(hashlib.sha1(mmap.mmap(read_file.fileno(), read_length, prot=mmap.PROT_READ,
                                                     offset=offset)).hexdigest())
        return output

    def copy_and_verify_file(self, input_file, output_file, supplied_checksums=None):
        copy(input_file, output_file)
        if not supplied_checksums:
            if stat(input_file).st_size > 0:
                input_checksums = self.file_checksums(input_file)
            else:
                input_checksums = 0
        else:
            input_checksums = supplied_checksums
        if stat(output_file).st_size > 0:
            output_checksums = self.file_checksums(output_file)
        else:
            output_checksums = 0
        if input_checksums == output_checksums:
            return input_checksums
        else:
            return None

    def clone_raw_vmdk(self, vmdk_file_path, output_folder, output_file_name=''):
        raw_file_name = ''
        replace_line = ''
        with open(vmdk_file_path, 'r') as vmdk_reader:
            for line in vmdk_reader:
                line_values = line.split(' ')
                if len(line_values) >= 4:
                    if (line_values[0] in {'RW', 'RDONLY', 'NOACCESS'}) and line_values[1].isdigit and line_values[2] \
                        in {'FLAT', 'SPARSE', 'ZERO', 'VMFS', 'VMFSSPARSE', 'VMFSRDM', 'VMFSRAW'} and \
                            line_values[-1].strip().isdigit():
                        raw_file_name = ' '.join(line_values[3:-1]).strip('"')
                        replace_line = line
                        break
        raw_file_path = path.join(path.split(vmdk_file_path)[0], raw_file_name)
        if output_file_name:
            new_raw_file_name = output_file_name + '.raw'
        else:
            new_raw_file_name = raw_file_name
        # REMOVE COMMENT - FOR DEBUGGING ONLY
        self.logfile.write('\tCopying {} to {}.\n'.format(raw_file_path, path.join(output_folder, new_raw_file_name)))
        self.copy_and_verify_file(raw_file_path, path.join(output_folder, new_raw_file_name))
        new_vmdk_file_name = re.sub('.raw$', '.vmdk', new_raw_file_name)
        new_vmdk_file_path = path.join(output_folder, new_vmdk_file_name)
        copy(vmdk_file_path, new_vmdk_file_path)
        if raw_file_name != output_file_name:
            file_contents = ''
            with open(new_vmdk_file_path, 'r') as vmdk_reader:
                for line in vmdk_reader:
                    if line == replace_line:
                        file_contents += line.replace(raw_file_name, new_raw_file_name)
                    else:
                        file_contents += line
            with open(new_vmdk_file_path, 'w') as vmdk_writer:
                vmdk_writer.writelines(file_contents)
        return [new_vmdk_file_name, new_raw_file_name]

    def create_loop_from_image(self, image_file):
        self.logfile.write('\tAttempting to setup {} as loopback\n'.format(image_file))
        loop_result = run('udisksctl loop-setup -r -f  {}'.format(image_file), shell=True, encoding='utf-8', 
                          stdout=PIPE)
        if loop_result != 0:
            self.logfile.write('\tMapped {} to loopback\n'.format(image_file))
            return loop_result.stdout
        self.logfile.write('\tFailed to map {} to loopback\n'.format(image_file))
        return False

    def mount_loop_image(self, loop_device):
        self.logfile.write('\tAttempting to mount {} safely from loopback\n'.format(loop_device))
        mount_result = run('udisksctl mount -b {}p2'.format(loop_device), shell=True, encoding='utf-8', stdout=PIPE)
        if mount_result != 0:
            self.logfile.write('\tMounted filesystem {}\n'.format(loop_device))
            return mount_result.stdout
        self.logfile.write('Failed to mount filesystem {}\n'.format(loop_device))
        return False

    def unmount_image(self, loop_device):
        self.logfile.write('Attempting to unmount loopback device {}\n'.format(loop_device))
        unmount_result = run('udisksctl unmount -b {}p2'.format(loop_device), shell=True,
                               encoding='utf-8', stdout=PIPE)
        if unmount_result != 0:
            self.logfile.write('\tUnmounted loopback device {}\n'.format(loop_device))
            return unmount_result.stdout
        self.logfile.write('Failed to unmount loopback device {}\n'.format(loop_device))
        return False

    def delete_loop(self, loop_device):
        self.logfile.write('Attempting to delete loopback {}\n'.format(loop_device))
        delete_loop_result = run('udisksctl loop-delete -b  {}'.format(loop_device), shell=True, encoding='utf-8', 
                                 stdout=PIPE)
        if delete_loop_result != 0:
            self.logfile.write('\tDeleted loopback {}\n'.format(loop_device))
            return delete_loop_result.stdout
        self.logfile.write('Failed to delete loopback {}\n'.format(loop_device))
        return False

    # This function uses udisksctl to automagically detect the underlying partition structure of the vm disk
    # and to map the disk image to loop back. The function then mounts partition 2 (the C:\ drive of the vm disk)
    # and returns the path of the mounted disk image. This function operates in userspace and does not require root
    def safemount_image(self, image_file):
        self.logfile.write('Attempting to safemount {}\n'.format(image_file))
        loop_device = ""
        mount_path = ""
        match = ""
        loop_device_string = self.create_loop_from_image(image_file)
        if loop_device_string:
            match = re.search(self.LOOPBACK_REGEX_STRING, loop_device_string)
        if match:
            loop_device = match.group(1)
            mount_string = self.mount_loop_image(loop_device)
            if mount_string:
                match = re.search(self.MOUNT_REGEX_STRING, mount_string)
                mount_path = match.group(1)
        if mount_path:
            self.logfile.write('Successfully safemounted {} as {}\n'.format(image_file, mount_path))
            return loop_device, mount_path
        elif loop_device:
            self.logfile.write('Mapped {} to loopback {}, but failed to mount\n'.format(image_file, loop_device))
            return loop_device
        return ''

    def get_uuid_from_virtual_disk(self, vm_disk_path):
        self.logfile.write('Attempting to determine UUID of virtual disk {}\n'.format(vm_disk_path))
        uuid_result = run('vboxmanage showhdinfo "{}"'.format(vm_disk_path), shell=True, encoding='utf-8',
                          stdout=PIPE)
        if uuid_result != 0:
            for line in uuid_result.stdout.split('\n'):
                if line[0:4] == 'UUID':
                    self.logfile.write('UUID {} associated with virtual disk {}\n'.format(line[6:].strip(),
                                                                                          vm_disk_path))
                    return line[6:].strip()
        self.logfile.write('Failed to obtain UUID associated with {}'.format(vm_disk_path))
        return ''
    
    # This function prunes appends a drive designation in Windows format (C:\blah\blah) to a format where
    # it can be appended to the local mount point i.e. /media/linux_user/blah/blah
    def windows_path_to_local_path(self, mount_path, file_path):
        split_path = file_path.split('\\')
        if split_path[0] == '':
            split_path = split_path[1:]
        if re.match(self.DRIVE_REGEX_STRING, split_path[0], re.IGNORECASE):
            split_path = split_path[1:]
        return path.join(mount_path, *split_path)
    
    # This function simply compiles the regexes used to convert registry entries into traversable paths
    def compile_registry_regexes(self):
        self.appdata_regex = re.compile(self.APPDATA_REGEX_STRING, re.IGNORECASE)
        self.local_appdata_regex = re.compile(self.LOCAL_APPDATA_REGEX_STRING, re.IGNORECASE)
        self.userprofile_regex = re.compile(self.USERPROFILE_REGEX_STRING, re.IGNORECASE)
        self.windir_regex = re.compile(self.WINDIR_REGEX_STRING, re.IGNORECASE)

    # This function takes the file paths supplied by Noriben and converts the special windows abbreviations (%VARIABLE%)
    # to traversable paths. Note: I had to hardcode double-slashes into the config file to get this to work correctly.
    def translate_windows_special_directories(self, noriben_output_list, config):
        if not self.appdata_regex:
            self.compile_registry_regexes()
        processed_output = []
        for line in noriben_output_list:
            processed_line = self.appdata_regex.sub(config['remote_appdata'], line[5])
            processed_line = self.windir_regex.sub(config['remote_windir'], processed_line)
            processed_line = self.userprofile_regex.sub(config['remote_userprofile'], processed_line)
            processed_line = self.local_appdata_regex.sub(config['remote_local_appdata'], processed_line)
            if processed_line:
                processed_output.append(processed_line)
        return processed_output
    
    # This function finds the Noriben timeline CSV stored in the noriben_path and ingests the File and Registry entries,
    # which are stored as a list of lists in files and registry_entries respectively.
    def ingest_noriben_data(self, noriben_path):
        self.logfile.write('Attempting to ingest Noriben CSV at {}\n'.format(noriben_path))
        timeline_re = re.compile(ForensicUtility.NORIBEN_TIMELINE_REGEX_STRING)
        timeline_path = ''
        exe_file = 'f'
        files = []
        registry_entries = []
        for file_object in listdir(noriben_path):
            if timeline_re.match(file_object):
                timeline_path = path.join(noriben_path, file_object)
                break
        if not timeline_path:
            self.logfile.write('Noriben CSV not found on {}\n'.format(noriben_path))
            raise IOError("Noriben CSV not found on {}".format(noriben_path))
        with open(timeline_path, 'r') as timeline_csv:
            data = timeline_csv.read()
            for line in data.split('\n'):
                output = line.split(',')
                if output:
                    if output[3] == 'WScript.exe':
                        exe_file = ntpath.basename(output[5])
                    if exe_file:
                        if output[1] == 'File':
                            files.append(output)
                        if output[1] == 'Registry':
                            registry_entries.append(output)
        if exe_file:
            self.logfile.write('Successfully ingested Noriben CSV at {}\n'.format(timeline_path))
            return files, registry_entries, exe_file
        else:
            self.logfile.write('Failed to find an appropriate CSV reflecting WScript at {}\n'.format(noriben_path))
            return None

    @staticmethod
    def unique_noriben_entries(list):
        unique_list = []
        for entry in list:
            if not unique_list:
                index_val = 1
            else:
                index_val = 0
                while True:
                    if index_val >= len(unique_list):
                        break
                    if entry[5] == unique_list[index_val][5]:
                        unique_list.pop(index_val)
                        break
                    else:
                        index_val += 1
            if len(unique_list) <= index_val:
                unique_list.append(entry.copy())
        return unique_list

    @staticmethod
    def classify_files_from_noriben_csv(files_list):
        unique_files = ForensicUtility.unique_noriben_entries(files_list)
        existing_list = []
        deleted_list = []
        for file in unique_files:
            if file[2] == "CreateFile":
                existing_list.append(file.copy())
            elif file[2] == "DeleteFile":
                deleted_list.append(file.copy())
        return(existing_list, deleted_list)

    @staticmethod
    def classify_registry_entries_from_noriben_csv(files_list):
        unique_entries = ForensicUtility.unique_noriben_entries(files_list)
        existing_list = []
        deleted_list = []
        for entry in unique_entries:
            if entry[2][0:6] == "RegSet":
                existing_list.append(entry.copy())
            elif entry[2][0:6] == "RegDel":
                deleted_list.append(entry.copy())
        return (existing_list, deleted_list)

    # This function maps registry key entries to the files containing the entries, adjusted for the mount point
    @staticmethod
    def get_registry_association(mount_path, key):
        split_key = key.split('\\')
        if split_key[0] == 'HKLM':
            if split_key[1].upper() == 'BCD' or split_key[1].upper() == 'BCD00000000':
                return path.join(mount_path, 'Windows/System32/config/BCD-Template'), ntpath.join(*split_key[2:])
            if split_key[1].upper() == 'COMPONENTS':
                return path.join(mount_path, 'Windows/System32/config/COMPONENTS'), ntpath.join(*split_key[2:])
            if split_key[1].upper() == 'SAM':
                return path.join(mount_path, 'Windows/System32/config/SAM'), ntpath.join(*split_key[2:])
            if split_key[1].upper() == 'SECURITY':
                return path.join(mount_path, 'Windows/System32/config/SECURITY'), ntpath.join(*split_key[2:])
            if split_key[1].upper() == 'SOFTWARE':
                return path.join(mount_path, 'Windows/System32/config/SOFTWARE'), ntpath.join(*split_key[2:])
            if split_key[1].upper() == 'SYSTEM':
                return path.join(mount_path, 'Windows/System32/config/SYSTEM'), ntpath.join(*split_key[2:])
        if split_key[0] == 'HKU':
            return path.join(mount_path, 'Windows/System32/config/DEFAULT'), ntpath.join(*split_key[2:])
        if split_key[0] == 'HKCU':
            return path.join(mount_path, 'Users/TestUser/NTUSER.DAT'), ntpath.join(*split_key[1:])
        if split_key[0] == 'HKCR':
            return path.join(mount_path, 'Windows/System32/config/SOFTWARE'), ntpath.join("Classes", *split_key[1:])
        if split_key[0] == 'HKCC':
            return path.join(mount_path, 'Windows/System32/config/SYSTEM'), \
                   ntpath.join("CurrentControlSet\Control\IDConfigDB", *split_key[2:])

    @staticmethod
    def get_registry_entry(registry_path, key, subkey):
        try:
            registry = Registry.Registry(registry_path)
            if key.startswith(registry.root().name()):
                key = registry.open(key.partition("\\")[2])
            else:
                key = registry.open(key)
                value = key.value(subkey)
        except Registry.RegistryKeyNotFoundException:
            return ''
        except Registry.RegistryValueNotFoundException:
            return ''
        return value.value()

    @staticmethod
    def generate_windows_noriben_path(remote_path):
        return ntpath.join(remote_path, 'Noriben', 'noriben.py')

    @staticmethod
    def generate_malware_path(remote_path, item):
        return ntpath.join(remote_path, 'Malware', item)

    def get_noriben_path(self):
        return self.noriben_path

    def get_malware_path(self):
        return self.malware_path

    def generate_script_paths(self, remote_path, item):
        self.noriben_path = self.generate_windows_noriben_path(remote_path)
        self.malware_path = self.generate_malware_path(remote_path, item)

    def create_windows_launcher(self, local_path, remote_path, item, boot_wait=15000,
                                command_wait=15000, infection_wait=60000):
        self.logfile.write('Creating Windows batchfile to execute malware sample in Noriben environment\n')
        self.generate_script_paths(remote_path, item)
        script_file_path = path.join(local_path, "launcher.vbs")
        script_file = open(script_file_path, "w")
        script_commands = 'shell_object.CurrentDirectory = "{}"\r\n'.format(remote_path)
        script_commands += 'Wscript.sleep {}\r\n'.format(boot_wait)
        script_commands += 'Set noriben = shell_object.Exec("python {}")\r\n'.format(self.noriben_path)
        script_commands += 'Wscript.sleep {}\r\n'.format(command_wait)
        script_commands += 'Set malware = shell_object.Exec("{}")\r\n'.format(self.malware_path)
        script_commands += 'Wscript.sleep {}\r\n'.format(infection_wait)
        script_file_text = self.script_start_text + script_commands + self.script_end_text
        script_file.write(script_file_text)
        script_file.close()
        self.logfile.write('Batchfile created at {}\n'.format(script_file_path))
        return script_file_path

    @staticmethod
    def analyze_chunk(search_path_list, target_folder, subsearch_size=1024*1024, read_segment_size=128*1024*1024):
        search_mmap_list = []
        search_sizes_list = []
        found_list = []
        for search_path in search_path_list:
            search_read_file = open(search_path, 'r+')
            search_mmap = mmap.mmap(search_read_file.fileno(), 0)
            search_mmap_list.append(search_mmap)
            search_sizes_list.append(path.getsize(search_path))
        subsearch_mmap_list = []
        for i in range(len(search_mmap_list)):
            if subsearch_size > search_sizes_list[i]:
                subsearch_mmap_list.append(search_mmap_list[i])
            else:
                subsearch_mmap_list.append(search_mmap_list[i][:subsearch_size])
        target_files = listdir(target_folder)
        for target_file in target_files:
            target_path = path.join(target_folder, target_file)
            print(target_path)
            target_segment_start = 0
            target_segment_end = read_segment_size
            target_length = path.getsize(target_path)
            subsearch_hits = []
            while target_segment_start + subsearch_size < target_length:
                if target_segment_end >= target_length:
                    target_read_file = open(target_path, 'r+')
                    target_mmap = mmap.mmap(target_read_file.fileno(), offset=target_segment_start,
                                            length=target_length - target_segment_start)
                else:
                    target_read_file = open(target_path, 'r+')
                    target_mmap = mmap.mmap(target_read_file.fileno(), offset=target_segment_start,
                                            length=(read_segment_size + subsearch_size))
                for i in range(len(subsearch_mmap_list)):
                    index = target_mmap.find(subsearch_mmap_list[i])
                    if index > -1:
                        subsearch_hits.append([i, index+target_segment_start])
                        print(subsearch_hits[-1])
                target_segment_start += read_segment_size
                target_segment_end += read_segment_size
                target_read_file.close()
            subsearch_hits.sort(key=lambda x: x[1])
            for i in range(len(search_mmap_list)):
                target_read_file = open(target_path, 'r+')
                target_read_length = search_sizes_list[i]
                for subsearch_hit in subsearch_hits:
                    if subsearch_hit[0] == i:
                        if subsearch_hit[1] + target_read_length < target_length:
                            offset_factor = subsearch_hit[1] // mmap.ALLOCATIONGRANULARITY
                            length_factor = target_read_length + subsearch_hit[1] % mmap.ALLOCATIONGRANULARITY
                            target_mmap = mmap.mmap(target_read_file.fileno(),
                                                    offset=offset_factor * mmap.ALLOCATIONGRANULARITY,
                                                    length=length_factor)
                            index = target_mmap.find(search_mmap_list[i])
                            if index > -1:
                                found_list.append([search_path_list[i], subsearch_hit[1], target_read_length])
                                print(found_list[-1])
